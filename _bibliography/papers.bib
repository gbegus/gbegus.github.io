---
---


@article{begus20,
author={Begu\v{s}, Ga\v{s}per},    
title={Generative Adversarial Phonology: Modeling Unsupervised Phonetic and Phonological Learning With Neural Networks},      	
journal={Frontiers in Artificial Intelligence},      	
volume={3},      
pages={44},     	
year={2020},      	  
url={https://www.frontiersin.org/article/10.3389/frai.2020.00044},
html={https://www.frontiersin.org/article/10.3389/frai.2020.00044},       
doi={10.3389/frai.2020.00044},      
issn={2624-8212},   
abstract={Training deep neural networks on well-understood dependencies in speech data can provide new insights into how they learn internal representations. This paper argues that acquisition of speech can be modeled as a dependency between random space and generated speech data in the Generative Adversarial Network architecture and proposes a methodology to uncover the network's internal representations that correspond to phonetic and phonological properties. The Generative Adversarial architecture is uniquely appropriate for modeling phonetic and phonological learning because the network is trained on unannotated raw acoustic data and learning is unsupervised without any language-specific assumptions or pre-assumed levels of abstraction. A Generative Adversarial Network was trained on an allophonic distribution in English, in which voiceless stops surface as aspirated word-initially before stressed vowels, except if preceded by a sibilant [s]. The network successfully learns the allophonic alternation: the network's generated speech signal contains the conditional distribution of aspiration duration. The paper proposes a technique for establishing the network's internal representations that identifies latent variables that correspond to, for example, presence of [s] and its spectral properties. By manipulating these variables, we actively control the presence of [s] and its frication amplitude in the generated outputs. This suggests that the network learns to use latent variables as an approximation of phonetic and phonological representations. Crucially, we observe that the dependencies learned in training extend beyond the training interval, which allows for additional exploration of learning representations. The paper also discusses how the network's architecture and innovative outputs resemble and differ from linguistic behavior in language acquisition, speech disorders, and speech errors, and how well-understood dependencies in speech data can help us interpret how neural networks learn their representations.},
pdf={begusGAP.pdf},
}

    
       



@inproceedings{begusscil,
    title = "Modeling unsupervised phonetic and phonological learning in {Generative Adversarial Phonology}",
    author = "Ga\v{s}per Begu\v{s}",
    booktitle = "Proceedings of the Society for Computation in Linguistics",
    year = "2020",
    url = "https://doi.org/10.7275/nbrf-1a27",
    html = "https://doi.org/10.7275/nbrf-1a27",
    doi = "10.7275/nbrf-1a27",
    pages = "138--148",
    pdf={begusScil.pdf},
    abstract={This paper models phonetic and phonological learning as a dependency between random space and generated speech data in the Generative Adversarial Neural network architecture and proposes a methodology to uncover the network’s internal representation that corresponds to phonetic and phonological features. A Generative Adversarial Network (Goodfellow et al. 2014; implemented as WaveGAN for acoustic data by Donahue et al. 2019) was trained on an allophonic distribution in English, where voiceless stops surface as aspirated word-initially before stressed vowels except if preceded by a sibilant [s]. The network successfully learns the allophonic alternation: the network’s generated speech signal contains the conditional distribution of aspiration duration. Additionally, the network generates innovative outputs for which no evidence is available in the training data, suggesting that the network segments continuous speech signal into units that can be productively recombined. The paper also proposes a technique for establishing the network’s internal representations. We identify latent variables that directly correspond to presence of [s] in the output. By manipulating these variables, we actively control the presence of [s], its frication amplitude, and spectral shape of the frication noise in the generated outputs.}
} 


@article{begus20phonology, 
title={Estimating historical probabilities of natural and unnatural processes}, 
volume={37}, 
DOI={10.1017/S0952675720000263}, 
number={4}, 
journal={Phonology}, 
publisher={Cambridge University Press}, 
author={Begu\v{s}, Ga\v{s}per}, 
year={2020}, 
pages={515–549},
pdf={begusPhonology.pdf},
html={https://www.cambridge.org/core/journals/phonology/article/estimating-historical-probabilities-of-natural-and-unnatural-processes/CAC83CE585B82836CBC0D3A4BCDF17EB},
abstract={This paper presents a technique for estimating the influences of channel bias on phonological typology. The technique, based on statistical bootstrapping, enables the estimation of historical probability, the probability that a synchronic alternation arises based on two diachronic factors: the number of sound changes required for an alternation to arise and their respective probabilities. I estimate historical probabilities of six attested and unattested alternations targeting the feature [voice], compare historical probabilities of these alternations, perform inferential statistics on the comparison and, to evaluate the performance of the channel bias approach, compare outputs of the diachronic model against the independently observed synchronic typology. The technique also identifies mismatches between the typological predictions of the analytic bias and channel bias approaches. By comparing these mismatches with the observed typology, this paper attempts to quantitatively evaluate the distinct contributions of the two influences on typology in a set of alternations targeting the feature [voice].},
}    


@article{begus19, 
title={Post-nasal devoicing and the blurring process},
volume={55}, DOI={10.1017/S002222671800049X}, 
number={4}, 
journal={Journal of Linguistics}, 
publisher={Cambridge University Press}, 
author={Begu\v{s}, Ga\v{s}per}, 
year={2019}, pages={689–753},
html={https://www.cambridge.org/core/journals/journal-of-linguistics/article/postnasal-devoicing-and-the-blurring-process/51B1F27754D14F8BF523B905FFFE3BA1/share/527b629e44500591b52700cd181c20b09e444e49},
abstract={This paper addresses one of the most contested issues in phonology: unnatural alternations. First, non-natural phonological processes are subdivided into unmotivated and unnatural. The central topic of the paper is an unnatural process: post-nasal devoicing (PND). I collect thirteen cases of PND and argue that in all reported cases, PND does not derive from a single unnatural sound change (as claimed in some individual accounts of the data), but rather from a combination of three sound changes, each of which is phonetically motivated. I present new evidence showing that the three stages are directly historically attested in the pre-history of Yaghnobi. Based on several discussed cases, I propose a new diachronic model for explaining unnatural phenomena called the Blurring Process and point to its advantages over competing approaches (hypercorrection, perceptual enhancement, and phonetic motivation). The Blurring Process establishes general diachronic conditions for unnatural synchronic processes and can be employed to explain unnatural processes beyond PND. Additionally, I provide a proof establishing the minimal sound changes required for an unmotivated/unnatural alternation to arise. The Blurring Process and Minimal Sound Change Requirement have implications for models of typology within the Channel Bias approach. This paper thus presents a first step toward the ultimate goal of quantifying the influences of Channel Bias on phonological typology.},
}

@INPROCEEDINGS{begusnazarov18, author= {Ga\v{s}per Begu\v{s}  and Aleksei Nazarov}, title= {Gradient trends against phonetic naturalness: The case of {Tarma Quechua}}, booktitle= {NELS 48: Proceedings of the Forty-Eighth Annual Meeting of the North East Linguistic Society}, year= {2018}, editor= {Sherry Hucklebridge and Max Nelson}, volume= {1}, pages= {73-86}, publisher= {GLSA University of Massachusetts Amherst}, address= {Amherst, MA}, isbn= {978-1727605792},
pdf={begusNazarovNels.pdf},
}


@inproceedings{begus, issn= {1042-1068}, abstract= {Identifying and modeling factors that influence typology has been one of the most contested issues in phonology with two major lines of thought emerging in this discussion: the Analytic Bias (AB) and Channel Bias (CB) approaches (Moreton 2008). Empirical evidence in favor of both approaches exists, yet very few attempts have been made to model them together. This paper aims to fill this gap and proposes a new MaxEnt-compatible model of phonological typology that models both AB and CB together. The first step towards a new model of typology is to establish quantitative models of each of the subcomponents: AB and CB. To encode the AB portion of the typology, we adopt Wilson's (2006) approach of differentiating variance in the prior of a MaxEnt model of phonological learning; to encode the CB portion, we adopt Beguš's (2016) new model of typology within CB that operates with Historical Probabilities of Alternations and an estimation method called Bootstrapping Sound Changes. This paper proposes a new model of typology that combines differentiating prior variance (AB; Wilson 2006) with estimating Historical Weights based on Historical Probabilities (CB; Beguš 2016), whereby both variables influence the typology. I further argue that this new model performs better than the current "split" models on the basis of two alternations, post-nasal voicing and devoicing, and point to future directions this line of research should take.}, booktitle= {Proceedings of the West Coast Conference on Formal Linguistics}, pages= {104}, publisher= {Cascadilla Press}, number= {35}, year= {2017}, title= {A Formal Model of Phonological Typology}, language= {eng}, address= {Tucson}, author= {Begu\v{s}, Ga\v{s}per}, keywords= {Phonology ; Bootstrapping ; Phonology ; Bias ; Voicing}, html= {http://search.proquest.com/docview/2244647771/},
pdf={begus_17_a-formal-model.pdf}}


@article{doi:10.1121/1.5007728,
author = {Begu\v{s}, Ga\v{s}per},
title = {Effects of ejective stops on preceding vowel duration},
journal = {Journal of the Acoustical Society of America},
volume = {142},
number = {4},
pages = {2168-2184},
year = {2017},
doi = {10.1121/1.5007728},
html = {https://doi.org/10.1121/1.5007728},
eprint = {https://doi.org/10.1121/1.5007728},
abstract = {One of the most widely studied observations in linguistic phonetics is that, all else being equal, vowels are longer before voiced than before voiceless obstruents. The causes of this phonetic generalization are, however, poorly understood and several competing explanations have been proposed. No studies have so far measured vowel duration before stops with yet another laryngeal feature: ejectives. This study fills this gap and presents results from an experiment that measures vowel duration before stops with all three laryngeal features in Georgian and models effects of both closure and voice onset time (VOT) on preceding vowel duration at the same time. The results show that vowels have significantly different durations before all three series of stops, voiced, ejective, and voiceless aspirated, even when closure and VOT durations are controlled for. The results also suggest that closure and VOT durations are inversely correlated with preceding vowel duration. These results combined bear several implications for the discussion of causes of vowel duration differences: the data support the hypotheses that claim that laryngeal gestures, temporal compensation, and closure velocity affect vowel duration. Some explanations, especially perceptual and airflow expenditure explanations, are considerably weakened by the results.},
pdf={begus_effects_of_ejective_stops_on_preceding_vowel_duration_01.pdf}
}

@INPROCEEDINGS{begus16, author= {Ga\v{s}per Begu\v{s}}, address= {Bremen}, title= {The phonetics of the independent svarita in Vedic}, booktitle= {Proceedings of the 26th Annual UCLA Indo-European Conference}, year= {2016}, editor= {Stephanie W. Jamison and H. Craig Melchert and Brent Harmon Vine and Angelo Mercado}, pages= {1-12}, publisher= {Hempen}, isbn= {9783944312323},
pdf={begus_the_phonetics_of_independent_svarita_in_vedic.pdf},
html={http://www.hempen-verlag.de/sprachwissenschaften/indogermanistik/ucla-proceedings/proceedings-of-the-26th-annual-ucla-indo-european-conference.html}}


@article{begus15,title = {A new rule in vedic metrics},journal = {Journal of the American Oriental Society},year = {2015},volume = {135},number = {3},pages = {541-550},author = {Ga\v{s}per Begu\v{s}},
pdf={begus_a_new_rule_in_vedic_metrics.pdf},
abstract={In this paper I propose a new rule of Vedic meter. The glides v and y are regularly lost before the corresponding high vowels ū̆ and ī̆ in Vedic. I argue that the word-initial glides v and y before the short vowels ŭ and ĭ still “make position” and that they should be restored for metrical purposes. This means that word-final syllables of the shape -V̆C should be scanned long if the following syllable begins with a u- or i- that goes back to *vu- or *yi-. This new rule has consequences for the general metrical shape of the Rigveda, as cadences previously scanned as irregular will be repaired to their canonical shape. The rule can also be employed as etymologically decisive for words that can potentially go back to forms with or without an initial glide.},
html={http://dx.doi.org/10.7817/jameroriesoci.135.3.541},
}


@article{Beguš2015,title = {The circumflex advancement in prekmurje {S}lovenian and {B}ednja {K}ajkavian},journal = {Zeitschrift f\"ur Slawistik},year = {2015},volume = {60},number = {1},pages = {33-44},author = {Ga\v{s}per Begu\v{s}},
html={http://dx.doi.org/10.1515/slaw-2015-0003},
abstract={The circumflex advancement is usually dated after the loss of the weak jers. However, this chronology has been questioned by Vermeer (1979) and Greenberg (1992, 1993), who claim the opposite: that the weak jers were lost after the advancement. They further propose the “non-advancement rule”, by which the circumflex does not advance if a weak jer follows. Their evidence comes almost exclusively from the l-participles of the accentual paradigm c, which have the initial accent in the two dialects. The article presents new data that argue against this proposal. It is shown that the circumflex regularly advances in words outside the category of l-participles despite the presence of a subsequent weak jer. Moreover, a new explanation is given for the initial accent in l-participles that better captures the data. }} 

@article{begus11,doi = {10.17161/sls.1808.7536},url = {https://doi.org/10.17161%2Fsls.1808.7536},year = "2011",month = {jan},publisher = {The University of Kansas},author = {Ga{\v{s}}per Begu{\v{s}}},title = {Relativna kronologija naglasnih pojavov govora {\v{Z}}irovske kotline poljanskega nare{\v{c}}ja},journal = {Slovenski jezik -- Slovene Linguistic Studies},
abstract={(translation) The Žiri Basin local dialect within the Poljane dialect evinces several special features in its structure and development. Based on the system described in Stanonik (1977), the author elucidates the system’s accentual changes from Common Slovene to the present state, and also presents some new discoveries concerning the system itself that help explain the phenomena more precisely. This description helps establish a relative chronology of the accentual phenomena, the resulting model of which is compared and contrasted with other explanations. Finally, the relative chronology of accentual changes is placed in the larger context of the development of Slovene.},
pdf={begus_relativna_kronologija_naglasnih_pojavov_govora_zirovske_kotline_poljanskega_narecja.pdf},
html={http://dx.doi.org/10.17161/SLS.1808.7536}} 

@article{begusCiw,
title = {CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with {G}enerative {A}dversarial {N}etworks},
journal = {Neural Networks},
volume = {139},
pages = {305-325},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021001052},
author = {Ga\v{s}per Begu\v{s}},
keywords = {Artificial intelligence, Generative adversarial networks, Speech, Lexical learning, Neural network interpretability, Acoustic word embedding},
abstract = {How can deep neural networks encode information that corresponds to words in human speech into raw acoustic data? This paper proposes two neural network architectures for modeling unsupervised lexical learning from raw acoustic inputs: ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN). These combine Deep Convolutional GAN architecture for audio data (WaveGAN; Donahue et al., 2019) with the information theoretic extension of GAN – InfoGAN (Chen et al., 2016) – and propose a new latent space structure that can model featural learning simultaneously with a higher level classification and allows for a very low-dimension vector representation of lexical items. In addition to the Generator and Discriminator networks, the architectures introduce a network that learns to retrieve latent codes from generated audio outputs. Lexical learning is thus modeled as emergent from an architecture that forces a deep neural network to output data such that unique information is retrievable from its acoustic outputs. The networks trained on lexical items from the TIMIT corpus learn to encode unique information corresponding to lexical items in the form of categorical variables in their latent space. By manipulating these variables, the network outputs specific lexical items. The network occasionally outputs innovative lexical items that violate training data, but are linguistically interpretable and highly informative for cognitive modeling and neural network interpretability. Innovative outputs suggest that phonetic and phonological representations learned by the network can be productively recombined and directly paralleled to productivity in human speech: a fiwGAN network trained on suit and dark outputs innovative start, even though it never saw start or even a [st] sequence in the training data. We also argue that setting latent featural codes to values well beyond training range results in almost categorical generation of prototypical lexical items and reveals underlying values of each latent code. Probing deep neural networks trained on well understood dependencies in speech bears implications for latent space interpretability and understanding how deep neural networks learn meaningful representations, as well as potential for unsupervised text-to-speech generation in the GAN framework.},
  pdf={BegusCiwgan.pdf},
  html={https://www.sciencedirect.com/science/article/pii/S0893608021001052},
}

@INPROCEEDINGS{begusCauc,
address = {Oxford},
author = {Ga\v{s}per Begu\v{s}},
booktitle = {The {O}xford Handbook of Languages of the {C}aucasus},
editor = {Maria Polinsky},
publisher = {Oxford University Press},
title = {Segmental Phonetics and Phonology in {C}aucasian languages},
year = {2021},
html = {https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780190690694.001.0001/oxfordhb-9780190690694-e-18?rskey=rXvdJS&result=1},
abstract={This chapter surveys the major topics of Caucasian segmental phonetics and phonology, focusing on topics with broader implications for general phonetic and phonological theory. The author first presents an acoustic phonetic analysis of phonemic inventories in the three Caucasian families, including both a review of recent instrumental data on the topic as well as a new analysis of new and existing experimental acoustic data. This analysis focuses on four primary topics: obstruents with different laryngeal features, typologically unusual segments, small vocalic inventories, and pharyngealization. The new acoustic data from a nonce-word experiment in Georgian and Megrelian offer evidence that aspiration in voiceless stops gradually, yet significantly shortens if another voiceless stop precedes the relevant one in a given word. The second part reviews analyses of Caucasian phonotactics, primarily of South Caucasian consonant clusters that play a crucial role in discussions on production versus perception in phonology. The chapter concludes with a collection of phonological alternations that have potential for future research on phonology.},
pdf={BegusCaucasian.pdf},
}
  


    
 
    
    
@article{begu2020deep,
    title={Deep sound change: Deep and iterative learning, convolutional neural networks, and language change},
    author={Ga\v{s}per Begu\v{s}},
    year={2021},
    eprint={2011.05463},
    archivePrefix={arXiv},
    journal={Preprint},
    primaryClass={cs.CL},
    html={https://arxiv.org/abs/2011.05463},
    abstract={This paper proposes a framework for modeling sound change that combines deep learning and iterative learning. Acquisition and transmission of speech is modeled by training generations of Generative Adversarial Networks (GANs) on unannotated raw speech data. The paper argues that several properties of sound change emerge from the proposed architecture. GANs (Goodfellow et al., 2014; Donahue et al., 2019) are uniquely appropriate for modeling language change because the networks are trained on raw unsupervised acoustic data, contain no language-specific features and, as argued in Beguš (2020), encode phonetic and phonological representations in their latent space and generate linguistically informative innovative data. The first generation of networks is trained on the relevant sequences in human speech from TIMIT. The subsequent generations are not trained on TIMIT, but on generated outputs from the previous generation and thus start learning from each other in an iterative learning task. The initial allophonic distribution is progressively being lost with each generation, likely due to pressures from the global distribution of aspiration in the training data. The networks show signs of a gradual shift in phonetic targets characteristic of a gradual phonetic sound change. At endpoints, the outputs superficially resemble a phonological change -- rule loss.},
    pdf={begusDeep.pdf},}
    
@article{begusLocal,
title = {Local and non-local dependency learning and emergence of rule-like representations in speech data by deep convolutional generative adversarial networks},
journal = {Computer Speech & Language},
volume = {71},
pages = {101244},
year = {2022},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2021.101244},
url = {https://www.sciencedirect.com/science/article/pii/S0885230821000516},
author = {Gašper Beguš},
keywords = {Neural networks, Behavioral experiments, Machine learning, Learning biases, Speech, Morphology},
abstract = {This paper argues that training Generative Adversarial Networks (GANs) on local and non-local dependencies in speech data offers insights into how deep neural networks discretize continuous data and how symbolic-like rule-based morphophonological processes emerge in a deep convolutional architecture. Acquisition of speech has recently been modeled as a dependency between latent space and data generated by GANs in Beguš (2020b), who models learning of a simple local allophonic distribution. We extend this approach to test learning of local and non-local phonological processes that include approximations of morphological processes. We further parallel outputs of the model to results of a behavioral experiment where human subjects are trained on the data used for training the GAN network. Four main conclusions emerge: (i) the networks provide useful information for computational models of speech acquisition even if trained on a comparatively small dataset of an artificial grammar learning experiment; (ii) local processes are easier to learn than non-local processes, which matches both behavioral data in human subjects and typology in the world’s languages. This paper also proposes (iii) how we can actively observe the network’s progress in learning and explore the effect of training steps on learning representations by keeping latent space constant across different training steps. Finally, this paper shows that (iv) the network learns to encode the presence of a prefix with a single latent variable; by interpolating this variable, we can actively observe the operation of a non-local phonological process. The proposed technique for retrieving learning representations has general implications for our understanding of how GANs discretize continuous speech data and suggests that rule-like generalizations in the training data are represented as an interaction between variables in the network’s latent space.},
html= {https://www.sciencedirect.com/science/article/pii/S0885230821000516},
	pdf={begusCSL.pdf}
}
   


@article{begusLanguage,
 title={Distinguishing cognitive from historical influences in phonology},
 html={https://muse.jhu.edu/article/849525},
  url={https://muse.jhu.edu/article/849525},
 abstract={Distinguishing cognitive influences from historical influences on human behavior has long been a disputed topic in behavioral sciences, including linguistics. The discussion is often complicated due to empirical evidence being consistent with both the cognitive and the historical approach. This article argues that phonology offers a unique test case for distinguishing historical and cognitive influences on grammar, and it proposes an experimental technique for testing the cognitive factor which controls for the historical factor. The article outlines a model called CATALYSIS for explaining how learnability influences phonological typology and presents experiments that simulate this process. Central to this discussion are unnatural phonological processes, that is, those that operate against universal phonetic tendencies and require complex historical trajectories in order to arise. By using statistical methods for estimating historical influences, mismatches in predictions between the cognitive and historical approaches to typology can be identified. By conducting artificial grammar learning experiments on processes for which the historical approach makes predictions that differ from those of the cognitive approach, the experimental technique proposed in this article controls for historical influences while testing cognitive factors. Results of online and fieldwork experiments on two languages, English and Slovenian, show that subjects prefer postnasal devoicing over postnasal fricative occlusion and devoicing in at least a subset of places of articulation, which aligns with the observed typology. The advantage of the proposed approach over existing experimental work is that it experimentally confirms a link between synchronic preferences and typology that is most likely not influenced by historical biases. Results suggest that complexity avoidance is the primary influence cognitive bias has on phonological systems in human languages. Applying this technique to further alternations should yield new information about those cognitive properties of phonological grammar that are not conflated with historical influences.},
 author={Begu\v{s}, Ga\v{s}per},
 year={2022},
 journal={Language},
 number={1},
 volume={98},
 pages={1-34},
 pdf={begusCatalysis1.pdf},
}


@article{begusTACL,
    author = {Begu\v{s}, Ga\v{s}per},
   title = "{Identity-Based Patterns in Deep Convolutional Networks: Generative Adversarial Phonology and Reduplication}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1180-1196},
    year = {2021},
    month = {10},
    abstract = "{This paper models unsupervised learning of an identity-based pattern (or copying) in speech called reduplication from raw continuous data with deep convolutional neural networks. We use the ciwGAN architecture (Beguš, 2021a) in which learning of meaningful representations in speech emerges from a requirement that the CNNs generate informative data. We propose a technique to wug-test CNNs trained on speech and, based on four generative tests, argue that the network learns to represent an identity-based pattern in its latent space. By manipulating only two categorical variables in the latent space, we can actively turn an unreduplicated form into a reduplicated form with no other substantial changes to the output in the majority of cases. We also argue that the network extends the identity-based pattern to unobserved data. Exploration of how meaningful representations of identity-based patterns emerge in CNNs and how the latent space variables outside of the training range correlate with identity-based patterns in the output has general implications for neural network interpretability.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00421},
    url = {https://doi.org/10.1162/tacl\_a\_00421},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00421/1971814/tacl\_a\_00421.pdf},
        pdf={begusTACL.pdf},
        html={https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00421/107834/Identity-Based-Patterns-in-Deep-Convolutional},
}


    
    
@article{begusJurgec21,
 title={Tone, stress, quantity, and quality: prosodic patterns and tonal wug-tests in \v{Z}iri Slovenian},
 html={https://psyarxiv.com/b7uj5},
 DOI={10.31234/osf.io/b7uj5},
 publisher={PsyArXiv},
 journal={Preprint},
 author={ Ga\v{s}per Begu\v{s} and Peter Jurgec},
 year={2021},
 abstract={The prosodic systems of the world’s languages vary widely in their complexity. In this paper, we report on a prosodic pattern in Žiri Slovenian, which displays an intricate set of interactions between tone, stress, quantity, and vowel quality. Some of the prosodic patterns are well motivated (e.g. the preference of High tone syllables to be stressed) while others lack phonetic or phonological motivation (e.g. the preference of long vowels to fall on the second syllable of a trochaic foot). Žiri also displays a rare case of interaction of prosody with vowel quality. To confirm productivity of the observed patterns, we conducted a wug experiment that tested the dependence of stress on vowel quality, word length, and tone. All in all, this paper brings forth new instances of phonetically unmotivated phonological processes at the suprasegmental level, which appear less discussed than at the segmental level. We also discuss methodological issues arising from artificial experiments on tonal processes.},
 month={May},
 pdf={begusJurgecZiri.pdf}
}


@article{begusNazarovBjorklundBaldoceda,
 title={Lexicon against Naturalness: Unnatural Gradient Phonotactic Restrictions in Tarma Quechua},
 html={psyarxiv.com/gw6vj},
 DOI={10.31234/osf.io/gw6vj},
 journal={Preprint},
 publisher={PsyArXiv},
 author={Ga\v{s}per Begu\v{s} and Aleksei Nazarov and Anna Bj\"orklund Blas Puente Baldoceda},
 year={2022},
 month={Aug},
  pdf={begusNazarovBjorklundBaldoceda.pdf},
  abstract={It has been shown in separate studies that phonological grammar can operate probabilistically and in phonetically unnatural directions. This paper examines whether phonological grammar can be both probabilistic and unnatural at the same time. We create a new corpus of Tarma Quechua vocabulary (based both on published and unpublished data), and argue that the unnatural probabilistic phonotactic trends in the Tarma Quechua lexicon are statistically significant and show clear signs of productivity, with evidence from loanword phonology and from morphophonological alternations. We also perform an acoustic analysis of existing recordings to confirm the phonetic status of the the unnatural gradient restriction. To our knowledge, this is the first report of a fully unnatural gradient phonotactic restriction on segmental structure. The existence of unnatural probabilistic phonology has broad theoretical consequences because it requires probabilistic approaches to phonology to derive unnatural patterns. While weighted constraint approaches are more powerful than non-probabilistic approaches, we argue that models without unnatural constraints are not sufficient for deriving Tarma Quechua data. We propose a new framework for evaluating phonological analyses using the goodness of fit measurement. This quantitative evaluation approach supports analyses with some, but not all unnatural constraints. We argue that the proposed approach can serve for evaluating other models and approaches},
}







@article{begusZhouModeling,
      title={Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data}, 
      author={Ga\v{s}per Begu\v{s} and Alan Zhou},
      year={2022},
      abstract={Human speakers encode information into raw speech which is then decoded by the listeners. This complex relationship between encoding (production) and decoding (perception) is often modeled separately. Here, we test how decoding of lexical and sublexical semantic information can emerge automatically from raw speech in unsupervised generative deep convolutional networks that combine both the production and perception principle. We introduce, to our knowledge, the most challenging objective in unsupervised lexical learning: an unsupervised network that must learn to assign unique representations for lexical items with no direct access to training data. We train several models (ciwGAN and fiwGAN by [1]) and test how the networks classify raw acoustic lexical items in the unobserved test data. Strong evidence in favor of lexical learning emerges. The architecture that combines the production and perception principles is thus able to learn to decode unique information from raw acoustic data in an unsupervised manner without ever accessing real training data. We propose a technique to explore lexical and sublexical learned representations in the classifier network. The results bear implications for both unsupervised speech synthesis and recognition as well as for unsupervised semantic modeling as language models increasingly bypass text and operate from raw acoustics.},
      html={https://arxiv.org/abs/2203.11476},
      journal={Accepted to Interspeech 2022},
      eprint={2203.11476},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      pdf={begusZhouModeling.pdf}
}

@article{begusZhouInterpretingCNN,
  doi = {10.48550/ARXIV.2104.09489},
 html = {https://arxiv.org/abs/2104.09489},
  author = {Ga\v{s}per Begu\v{s} and Zhou, Alan},
  keywords = {Sound (cs.SD), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {Interpreting intermediate convolutional layers of CNNs trained on raw speech},
journal={Accepted to IEEE/ACM Transactions on Audio Speech and Language Processing},
pdf={begusZhouInterpretingCNN.pdf},
year = {2022},
      abstract={This paper presents a technique to interpret and visualize intermediate layers in generative CNNs trained on raw speech data in an unsupervised manner. We argue that averaging over feature maps after ReLU activation in each transpose convolutional layer yields interpretable time-series data. This technique allows for acoustic analysis of intermediate layers that parallels the acoustic analysis of human speech data: we can extract F0, intensity, duration, formants, and other acoustic properties from intermediate layers in order to test where and how CNNs encode various types of information. We further combine this technique with linear interpolation of a model's latent space to show a causal relationship between individual variables in the latent space and activations in a model's intermediate convolutional layers. In particular, observing the causal effect between linear interpolation and the resulting changes in intermediate layers can reveal how individual latent variables get transformed into spikes in activation in intermediate layers. We train and probe internal representations of two models -- a bare WaveGAN architecture and a ciwGAN extension which forces the Generator to output informative data and results in the emergence of linguistically meaningful representations. Interpretation and visualization is performed for three basic acoustic properties of speech: periodic vibration (corresponding to vowels), aperiodic noise vibration (corresponding to fricatives), and silence (corresponding to stops). The proposal also allows testing of higher-level morphophonological alternations such as reduplication (copying). In short, using the proposed technique, we can analyze how linguistically meaningful units in speech get encoded in each convolutional layer of a generative neural network.},
}



@article{begusZhou2021interpreting,
      title={Interpreting intermediate convolutional layers of CNNs trained on raw speech}, 
      author={Ga\v{s}per Begu\v{s} and Alan Zhou},
      year={2021},
      eprint={2104.09489},
      html={https://arxiv.org/abs/2104.09489},
      journal={Preprint},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      abstract={This paper presents a technique to interpret and visualize intermediate layers in CNNs trained on raw speech data in an unsupervised manner. We show that averaging over feature maps after ReLU activation in each convolutional layer yields interpretable time-series data. The proposed technique enables acoustic analysis of intermediate convolutional layers. To uncover how meaningful representation in speech gets encoded in intermediate layers of CNNs, we manipulate individual latent variables to marginal levels outside of the training range. We train and probe internal representations on two models -- a bare WaveGAN architecture and a ciwGAN extension which forces the Generator to output informative data and results in emergence of linguistically meaningful representations. Interpretation and visualization is performed for three basic acoustic properties of speech: periodic vibration (corresponding to vowels), aperiodic noise vibration (corresponding to fricatives), and silence (corresponding to stops). We also argue that the proposed technique allows acoustic analysis of intermediate layers that parallels the acoustic analysis of human speech data: we can extract F0, intensity, duration, formants, and other acoustic properties from intermediate layers in order to test where and how CNNs encode various types of information. The models are trained on two speech processes with different degrees of complexity: a simple presence of [s] and a computationally complex presence of reduplication (copied material). Observing the causal effect between interpolation and the resulting changes in intermediate layers can reveal how individual variables get transformed into spikes in activation in intermediate layers. Using the proposed technique, we can analyze how linguistically meaningful units in speech get encoded in different convolutional layers.},
            pdf={begusZhouInterpreting.pdf}
}

@article{andreas2021cetacean,
title = {Toward understanding the communication in sperm whales},
journal = {iScience},
volume = {25},
number = {6},
pages = {104393},
year = {2022},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2022.104393},
url = {https://www.sciencedirect.com/science/article/pii/S2589004222006642},
html = {https://www.sciencedirect.com/science/article/pii/S2589004222006642},
author = {J. Andreas and G. Begu\v{s} and  M. Bronstein and R. Diamant and D. Delaney and S. Gero and S. Goldwasser and D. Gruber and S. {de Haas} and P. Malkin and N. Pavlov and R. Payne and G. Petri and D. Rus and P. Sharma and D. Tchernov and P. T{\o}nnesen and A. Torralba and D. Vogt and R. Wood},
keywords = {Ethology, Artificial intelligence, Natural language processing, Linguistics},
abstract = {Machine learning has been advancing dramatically over the past decade. Most strides are human-based applications due to the availability of large-scale datasets; however, opportunities are ripe to apply this technology to more deeply understand non-human communication. We detail a scientific roadmap for advancing the understanding of communication of whales that can be built further upon as a template to decipher other forms of animal and non-human communication. Sperm whales, with their highly developed neuroanatomical features, cognitive abilities, social structures, and discrete click-based encoding make for an excellent model for advanced tools that can be applied to other animals in the future. We outline the key elements required for the collection and processing of massive datasets, detecting basic communication units and language-like higher-level structures, and validating models through interactive playback experiments. The technological capabilities developed by such an undertaking hold potential for cross-applications in broader communities investigating non-human communication and behavioral research.},
pdf={iscience.pdf},
}

@INPROCEEDINGS{begusZhouIcassp,
  author={Ga\v{s}per Begu\v{s} and Alan Zhou},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Interpreting Intermediate Convolutional Layers In Unsupervised Acoustic Word Classification}, 
  year={2022},
  volume={},
  number={},
  pages={8207-8211},
  doi={10.1109/ICASSP43922.2022.9746849},
  html={https://ieeexplore.ieee.org/document/9746849},
  pdf={begusZhou.pdf},
  abstract={Understanding how deep convolutional neural networks classify data has been subject to extensive research. This paper proposes a technique to visualize and interpret intermediate layers of unsupervised deep convolutional networks by averaging over individual feature maps in each convolutional layer and inferring underlying distributions of words with non-linear regression techniques. A GAN-based architecture (ciwGAN [1]) that includes a Generator, a Discriminator, and a classifier was trained on unlabeled sliced lexical items from TIMIT. The training process results in a deep convolutional network that learns to classify words into discrete classes only from the requirement of the Generator to output informative data. This classifier network has no access to the training data – only to the generated data. We propose a technique to visualize individual convolutional layers in the classifier that yields highly informative time-series data for each convolutional layer and apply it to unobserved test data. Using non-linear regression, we infer underlying distributions for each word which allows us to analyze both absolute values and shapes of individual words at different convolutional layers, as well as perform hypothesis testing on their acoustic properties. The technique also allows us to test individual phone contrasts and how they are represented at each layer.}
  }
    
    
    
@article {begusZhouZhao22,
	author = {Begu\v{s}, Ga\v{s}per and Zhou, Alan and Zhao, T. Christina},
	title = {Encoding of speech in convolutional layers and the brain stem based on language experience},
	elocation-id = {2022.01.03.474864},
	year = {2022},
	doi = {10.1101/2022.01.03.474864},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Comparing artificial neural networks (ANNs) with outputs of brain imaging techniques has recently seen substantial advances in (computer) vision and text-based language models. Here, we propose a framework to compare biological and artificial neural computations of spoken language representations and propose several new challenges to this paradigm. Using a technique proposed by Begu{\v s} and Zhou (2021b), we can analyze encoding of any acoustic property in intermediate convolutional layers of an artificial neural network. This allows us to test similarities in speech encoding between the brain and artificial neural networks in a way that is more interpretable than the majority of existing proposals that focus on correlations and supervised models. We introduce fully unsupervised deep generative models (the Generative Adversarial Network architecture) trained on raw speech to the brain-and-ANN-comparison paradigm, which enable testing of both the production and perception principles in human speech. We present a framework that parallels electrophysiological experiments measuring complex Auditory Brainstem Response (cABR) in human brain with intermediate layers in deep convolutional networks. We compared peak latency in cABR relative to the stimulus in the brain stem experiment, and in intermediate convolutional layers relative to the input/output in deep convolutional networks. We also examined and compared the effect of prior language exposure on the peak latency in cABR, and in intermediate convolutional layers of a phonetic property. Specifically, the phonetic property (i.e., VOT =10 ms) is perceived differently by English vs. Spanish speakers as voiced (e.g. [ba]) vs voiceless (e.g. [pa]). Critically, the cABR peak latency to the VOT phonetic property is different between English and Spanish speakers, and peak latency in intermediate convolutional layers is different between English-trained and Spanish-trained computational models. Substantial similarities in peak latency encoding between the human brain and intermediate convolutional networks emerge based on results from eight trained networks (including a replication experiment). The proposed technique can be used to compare encoding between the human brain and intermediate convolutional layers for any acoustic property.Competing Interest StatementThe authors have declared no competing interest.},
	html = {https://www.biorxiv.org/content/early/2022/01/04/2022.01.03.474864},
	eprint = {https://www.biorxiv.org/content/early/2022/01/04/2022.01.03.474864.full.pdf},
	journal = {bioRxiv},
	   pdf={begusZhouZhao.pdf},
}

