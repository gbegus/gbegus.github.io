<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Gasper Begus | publications</title>
<meta name="description" content="Assistant Professor of Linguistics at UC Berkeley. Deep learning and computation in speech, phonetics, phonology.
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/css/mdb.min.css" integrity="sha256-/SwJ2GDcEt5382i8zqDwl36VJGECxEoIcBIuoLmLR4g=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"  integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Open Graph -->


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
      
      
      
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Gasper</span>   Begus
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/CV/">
                CV
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching/advising
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/videos/">
                videos
                
              </a>
          </li>
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">publications in reversed chronological order.</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusZhouZhao22" class="col-sm-8">
    
      <span class="title">Encoding of speech in convolutional layers and the brain stem based on language experience</span>
      <span class="author">
        
          
            
              
                
                  Beguš, Gašper,
                
              
            
          
        
          
            
              
                
                  Zhou, Alan,
                
              
            
          
        
          
            
              
                
                  and Zhao, T. Christina
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>bioRxiv</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.biorxiv.org/content/early/2022/01/04/2022.01.03.474864" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusZhouZhao.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Comparing artificial neural networks (ANNs) with outputs of brain imaging techniques has recently seen substantial advances in (computer) vision and text-based language models. Here, we propose a framework to compare biological and artificial neural computations of spoken language representations and propose several new challenges to this paradigm. Using a technique proposed by Beguš and Zhou (2021b), we can analyze encoding of any acoustic property in intermediate convolutional layers of an artificial neural network. This allows us to test similarities in speech encoding between the brain and artificial neural networks in a way that is more interpretable than the majority of existing proposals that focus on correlations and supervised models. We introduce fully unsupervised deep generative models (the Generative Adversarial Network architecture) trained on raw speech to the brain-and-ANN-comparison paradigm, which enable testing of both the production and perception principles in human speech. We present a framework that parallels electrophysiological experiments measuring complex Auditory Brainstem Response (cABR) in human brain with intermediate layers in deep convolutional networks. We compared peak latency in cABR relative to the stimulus in the brain stem experiment, and in intermediate convolutional layers relative to the input/output in deep convolutional networks. We also examined and compared the effect of prior language exposure on the peak latency in cABR, and in intermediate convolutional layers of a phonetic property. Specifically, the phonetic property (i.e., VOT =10 ms) is perceived differently by English vs. Spanish speakers as voiced (e.g. [ba]) vs voiceless (e.g. [pa]). Critically, the cABR peak latency to the VOT phonetic property is different between English and Spanish speakers, and peak latency in intermediate convolutional layers is different between English-trained and Spanish-trained computational models. Substantial similarities in peak latency encoding between the human brain and intermediate convolutional networks emerge based on results from eight trained networks (including a replication experiment). The proposed technique can be used to compare encoding between the human brain and intermediate convolutional layers for any acoustic property.Competing Interest StatementThe authors have declared no competing interest.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusLocal" class="col-sm-8">
    
      <span class="title">Local and non-local dependency learning and emergence of rule-like representations in speech data by deep convolutional generative adversarial networks</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Computer Speech &amp; Language</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.sciencedirect.com/science/article/pii/S0885230821000516" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusCSL.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper argues that training Generative Adversarial Networks (GANs) on local and non-local dependencies in speech data offers insights into how deep neural networks discretize continuous data and how symbolic-like rule-based morphophonological processes emerge in a deep convolutional architecture. Acquisition of speech has recently been modeled as a dependency between latent space and data generated by GANs in Beguš (2020b), who models learning of a simple local allophonic distribution. We extend this approach to test learning of local and non-local phonological processes that include approximations of morphological processes. We further parallel outputs of the model to results of a behavioral experiment where human subjects are trained on the data used for training the GAN network. Four main conclusions emerge: (i) the networks provide useful information for computational models of speech acquisition even if trained on a comparatively small dataset of an artificial grammar learning experiment; (ii) local processes are easier to learn than non-local processes, which matches both behavioral data in human subjects and typology in the world’s languages. This paper also proposes (iii) how we can actively observe the network’s progress in learning and explore the effect of training steps on learning representations by keeping latent space constant across different training steps. Finally, this paper shows that (iv) the network learns to encode the presence of a prefix with a single latent variable; by interpolating this variable, we can actively observe the operation of a non-local phonological process. The proposed technique for retrieving learning representations has general implications for our understanding of how GANs discretize continuous speech data and suggests that rule-like generalizations in the training data are represented as an interaction between variables in the network’s latent space.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusLanguage" class="col-sm-8">
    
      <span class="title">Distinguishing cognitive from historical influences in phonology</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Language</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://muse.jhu.edu/article/849525" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusCatalysis1.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Distinguishing cognitive influences from historical influences on human behavior has long been a disputed topic in behavioral sciences, including linguistics. The discussion is often complicated due to empirical evidence being consistent with both the cognitive and the historical approach. This article argues that phonology offers a unique test case for distinguishing historical and cognitive influences on grammar, and it proposes an experimental technique for testing the cognitive factor which controls for the historical factor. The article outlines a model called CATALYSIS for explaining how learnability influences phonological typology and presents experiments that simulate this process. Central to this discussion are unnatural phonological processes, that is, those that operate against universal phonetic tendencies and require complex historical trajectories in order to arise. By using statistical methods for estimating historical influences, mismatches in predictions between the cognitive and historical approaches to typology can be identified. By conducting artificial grammar learning experiments on processes for which the historical approach makes predictions that differ from those of the cognitive approach, the experimental technique proposed in this article controls for historical influences while testing cognitive factors. Results of online and fieldwork experiments on two languages, English and Slovenian, show that subjects prefer postnasal devoicing over postnasal fricative occlusion and devoicing in at least a subset of places of articulation, which aligns with the observed typology. The advantage of the proposed approach over existing experimental work is that it experimentally confirms a link between synchronic preferences and typology that is most likely not influenced by historical biases. Results suggest that complexity avoidance is the primary influence cognitive bias has on phonological systems in human languages. Applying this technique to further alternations should yield new information about those cognitive properties of phonological grammar that are not conflated with historical influences.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusNazarovBjorklundBaldoceda" class="col-sm-8">
    
      <span class="title">Lexicon against Naturalness: Unnatural Gradient Phonotactic Restrictions in Tarma Quechua</span>
      <span class="author">
        
          
            
              
                
                  Beguš, Gašper,
                
              
            
          
        
          
            
              
                
                  Nazarov, Aleksei,
                
              
            
          
        
          
            
              
                
                  Björklund, Anna,
                
              
            
          
        
          
            
              
                
                  and Baldoceda, Blas Puente
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>Preprint</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://psyarxiv.com/gw6vj" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusNazarovBjorklundBaldoceda.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>It has been shown in separate studies that phonological grammar can operate probabilistically and in phonetically unnatural directions. This paper examines whether phonological grammar can be both probabilistic and unnatural at the same time. We create a new corpus of Tarma Quechua vocabulary (based both on published and unpublished data), and argue that the unnatural probabilistic phonotactic trends in the Tarma Quechua lexicon are statistically significant and show clear signs of productivity, with evidence from loanword phonology and from morphophonological alternations. We also perform an acoustic analysis of existing recordings to confirm the phonetic status of the the unnatural gradient restriction. To our knowledge, this is the first report of a fully unnatural gradient phonotactic restriction on segmental structure. The existence of unnatural probabilistic phonology has broad theoretical consequences because it requires probabilistic approaches to phonology to derive unnatural patterns. While weighted constraint approaches are more powerful than non-probabilistic approaches, we argue that models without unnatural constraints are not sufficient for deriving Tarma Quechua data. We propose a new framework for evaluating phonological analyses using the goodness of fit measurement. This quantitative evaluation approach supports analyses with some, but not all unnatural constraints. We argue that the proposed approach can serve for evaluating other models and approaches</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusZhouInterpretingCNN" class="col-sm-8">
    
      <span class="title">Interpreting Intermediate Convolutional Layers of Generative CNNs Trained on Waveforms</span>
      <span class="author">
        
          
            
              
                
                  Beguš, Gašper,
                
              
            
          
        
          
            
              
                
                  and Zhou, Alan
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://doi.org/10.1109/TASLP.2022.3209938" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusZhouInterpretingCNN.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper presents a technique to interpret and visualize intermediate layers in generative CNNs trained on raw speech data in an unsupervised manner. We argue that averaging over feature maps after ReLU activation in each transpose convolutional layer yields interpretable time-series data. This technique allows for acoustic analysis of intermediate layers that parallels the acoustic analysis of human speech data: we can extract F0, intensity, duration, formants, and other acoustic properties from intermediate layers in order to test where and how CNNs encode various types of information. We further combine this technique with linear interpolation of a model’s latent space to show a causal relationship between individual variables in the latent space and activations in a model’s intermediate convolutional layers. In particular, observing the causal effect between linear interpolation and the resulting changes in intermediate layers can reveal how individual latent variables get transformed into spikes in activation in intermediate layers. We train and probe internal representations of two models – a bare WaveGAN architecture and a ciwGAN extension which forces the Generator to output informative data and results in the emergence of linguistically meaningful representations. Interpretation and visualization is performed for three basic acoustic properties of speech: periodic vibration (corresponding to vowels), aperiodic noise vibration (corresponding to fricatives), and silence (corresponding to stops). The proposal also allows testing of higher-level morphophonological alternations such as reduplication (copying). In short, using the proposed technique, we can analyze how linguistically meaningful units in speech get encoded in each convolutional layer of a generative neural network.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="andreas2021cetacean" class="col-sm-8">
    
      <span class="title">Toward understanding the communication in sperm whales</span>
      <span class="author">
        
          
            
              
                
                  Andreas, J.,
                
              
            
          
        
          
            
              
                
                  Beguš, G.,
                
              
            
          
        
          
            
              
                
                  Bronstein, M.,
                
              
            
          
        
          
            
              
                
                  Diamant, R.,
                
              
            
          
        
          
            
              
                
                  Delaney, D.,
                
              
            
          
        
          
            
              
                
                  Gero, S.,
                
              
            
          
        
          
            
              
                
                  Goldwasser, S.,
                
              
            
          
        
          
            
              
                
                  Gruber, D.,
                
              
            
          
        
          
            
              
                
                  de Haas, S.,
                
              
            
          
        
          
            
              
                
                  Malkin, P.,
                
              
            
          
        
          
            
              
                
                  Pavlov, N.,
                
              
            
          
        
          
            
              
                
                  Payne, R.,
                
              
            
          
        
          
            
              
                
                  Petri, G.,
                
              
            
          
        
          
            
              
                
                  Rus, D.,
                
              
            
          
        
          
            
              
                
                  Sharma, P.,
                
              
            
          
        
          
            
              
                
                  Tchernov, D.,
                
              
            
          
        
          
            
              
                
                  Tønnesen, P.,
                
              
            
          
        
          
            
              
                
                  Torralba, A.,
                
              
            
          
        
          
            
              
                
                  Vogt, D.,
                
              
            
          
        
          
            
              
                
                  and Wood, R.
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>iScience</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.sciencedirect.com/science/article/pii/S2589004222006642" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/iscience.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Machine learning has been advancing dramatically over the past decade. Most strides are human-based applications due to the availability of large-scale datasets; however, opportunities are ripe to apply this technology to more deeply understand non-human communication. We detail a scientific roadmap for advancing the understanding of communication of whales that can be built further upon as a template to decipher other forms of animal and non-human communication. Sperm whales, with their highly developed neuroanatomical features, cognitive abilities, social structures, and discrete click-based encoding make for an excellent model for advanced tools that can be applied to other animals in the future. We outline the key elements required for the collection and processing of massive datasets, detecting basic communication units and language-like higher-level structures, and validating models through interactive playback experiments. The technological capabilities developed by such an undertaking hold potential for cross-applications in broader communities investigating non-human communication and behavioral research.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusZhouIcassp" class="col-sm-8">
    
      <span class="title">Interpreting Intermediate Convolutional Layers In Unsupervised Acoustic Word Classification</span>
      <span class="author">
        
          
            
              
                
                  Beguš, Gašper,
                
              
            
          
        
          
            
              
                
                  and Zhou, Alan
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://ieeexplore.ieee.org/document/9746849" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusZhou.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Understanding how deep convolutional neural networks classify data has been subject to extensive research. This paper proposes a technique to visualize and interpret intermediate layers of unsupervised deep convolutional networks by averaging over individual feature maps in each convolutional layer and inferring underlying distributions of words with non-linear regression techniques. A GAN-based architecture (ciwGAN [1]) that includes a Generator, a Discriminator, and a classifier was trained on unlabeled sliced lexical items from TIMIT. The training process results in a deep convolutional network that learns to classify words into discrete classes only from the requirement of the Generator to output informative data. This classifier network has no access to the training data – only to the generated data. We propose a technique to visualize individual convolutional layers in the classifier that yields highly informative time-series data for each convolutional layer and apply it to unobserved test data. Using non-linear regression, we infer underlying distributions for each word which allows us to analyze both absolute values and shapes of individual words at different convolutional layers, as well as perform hypothesis testing on their acoustic properties. The technique also allows us to test individual phone contrasts and how they are represented at each layer.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus22_interspeech" class="col-sm-8">
    
      <span class="title">Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data</span>
      <span class="author">
        
          
            
              
                
                  Beguš, Gašper,
                
              
            
          
        
          
            
              
                
                  and Zhou, Alan
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proc. Interspeech 2022</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.isca-speech.org/archive/interspeech_2022/begus22_interspeech.html" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusZhouInterspeech.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Human speakers encode information into raw speech which is then decoded by the listeners. This complex relationship between encoding (production) and decoding (perception) is often modeled separately. Here, we test how encoding and decoding of lexical semantic information can emerge automatically from raw speech in unsupervised generative deep convolutional networks that combine the production and perception principles of speech. We introduce, to our knowledge, the most challenging objective in unsupervised lexical learning: a network that must learn unique representations for lexical items with no direct access to training data. We train several models (ciwGAN and fiwGAN [1]) and test how the networks classify acoustic lexical items in unobserved test data. Strong evidence in favor of lexical learning and a causal relationship between latent codes and meaningful sublexical units emerge. The architecture that combines the production and perception principles is thus able to learn to decode unique information from raw acoustic data without accessing real training data directly. We propose a technique to explore lexical (holistic) and sublexical (featural) learned representations in the classifier network. The results bear implications for unsupervised speech technology, as well as for unsupervised semantic modeling as language models increasingly bypass text and operate from raw acoustics.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusetalArticulationGAN" class="col-sm-8">
    
      <span class="title">Articulation GAN: Unsupervised modeling of articulatory learning</span>
      <span class="author">
        
          
            
              
                
                  Beguš, Gašper,
                
              
            
          
        
          
            
              
                
                  Zhou, Alan,
                
              
            
          
        
          
            
              
                
                  Wu, Peter,
                
              
            
          
        
          
            
              
                
                  and Anumanchipalli, Gopala K.
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>arXiv</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2210.15173" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/BegusetalArticulationGAN.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Generative deep neural networks are widely used for speech synthesis, but most existing models directly generate waveforms or spectral outputs. Humans, however, produce speech by controlling articulators, which results in the production of speech sounds through physical properties of sound propagation. We propose a new unsupervised generative model of speech production/synthesis that includes articulatory representations and thus more closely mimics human speech production. We introduce the Articulatory Generator to the Generative Adversarial Network paradigm. The Articulatory Generator needs to learn to generate articulatory representations (electromagnetic articulography or EMA) in a fully unsupervised manner without ever accessing EMA data. A separate pre-trained physical model (ema2wav) then transforms the generated EMA representations to speech waveforms, which get sent to the Discriminator for evaluation. Articulatory analysis of the generated EMA representations suggests that the network learns to control articulators in a manner that closely follows human articulators during speech production. Acoustic analysis of the outputs suggest that the network learns to generate words that are part of training data as well as novel innovative words that are absent from training data. Our proposed architecture thus allows modeling of articulatory learning with deep neural networks from raw audio inputs in a fully unsupervised manner. We additionally discuss implications of articulatory representations for cognitive models of human language and speech technology in general.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusCiw" class="col-sm-8">
    
      <span class="title">CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Neural Networks</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.sciencedirect.com/science/article/pii/S0893608021001052" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/BegusCiwgan.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>How can deep neural networks encode information that corresponds to words in human speech into raw acoustic data? This paper proposes two neural network architectures for modeling unsupervised lexical learning from raw acoustic inputs: ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN). These combine Deep Convolutional GAN architecture for audio data (WaveGAN; Donahue et al., 2019) with the information theoretic extension of GAN – InfoGAN (Chen et al., 2016) – and propose a new latent space structure that can model featural learning simultaneously with a higher level classification and allows for a very low-dimension vector representation of lexical items. In addition to the Generator and Discriminator networks, the architectures introduce a network that learns to retrieve latent codes from generated audio outputs. Lexical learning is thus modeled as emergent from an architecture that forces a deep neural network to output data such that unique information is retrievable from its acoustic outputs. The networks trained on lexical items from the TIMIT corpus learn to encode unique information corresponding to lexical items in the form of categorical variables in their latent space. By manipulating these variables, the network outputs specific lexical items. The network occasionally outputs innovative lexical items that violate training data, but are linguistically interpretable and highly informative for cognitive modeling and neural network interpretability. Innovative outputs suggest that phonetic and phonological representations learned by the network can be productively recombined and directly paralleled to productivity in human speech: a fiwGAN network trained on suit and dark outputs innovative start, even though it never saw start or even a [st] sequence in the training data. We also argue that setting latent featural codes to values well beyond training range results in almost categorical generation of prototypical lexical items and reveals underlying values of each latent code. Probing deep neural networks trained on well understood dependencies in speech bears implications for latent space interpretability and understanding how deep neural networks learn meaningful representations, as well as potential for unsupervised text-to-speech generation in the GAN framework.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusCauc" class="col-sm-8">
    
      <span class="title">Segmental Phonetics and Phonology in Caucasian languages</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>In The Oxford Handbook of Languages of the Caucasus</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780190690694.001.0001/oxfordhb-9780190690694-e-18?rskey=rXvdJS&amp;result=1" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/BegusCaucasian.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This chapter surveys the major topics of Caucasian segmental phonetics and phonology, focusing on topics with broader implications for general phonetic and phonological theory. The author first presents an acoustic phonetic analysis of phonemic inventories in the three Caucasian families, including both a review of recent instrumental data on the topic as well as a new analysis of new and existing experimental acoustic data. This analysis focuses on four primary topics: obstruents with different laryngeal features, typologically unusual segments, small vocalic inventories, and pharyngealization. The new acoustic data from a nonce-word experiment in Georgian and Megrelian offer evidence that aspiration in voiceless stops gradually, yet significantly shortens if another voiceless stop precedes the relevant one in a given word. The second part reviews analyses of Caucasian phonotactics, primarily of South Caucasian consonant clusters that play a crucial role in discussions on production versus perception in phonology. The chapter concludes with a collection of phonological alternations that have potential for future research on phonology.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begu2020deep" class="col-sm-8">
    
      <span class="title">Deep sound change: Deep and iterative learning, convolutional neural networks, and language change</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Preprint</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2011.05463" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusDeep.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper proposes a framework for modeling sound change that combines deep learning and iterative learning. Acquisition and transmission of speech is modeled by training generations of Generative Adversarial Networks (GANs) on unannotated raw speech data. The paper argues that several properties of sound change emerge from the proposed architecture. GANs (Goodfellow et al., 2014; Donahue et al., 2019) are uniquely appropriate for modeling language change because the networks are trained on raw unsupervised acoustic data, contain no language-specific features and, as argued in Beguš (2020), encode phonetic and phonological representations in their latent space and generate linguistically informative innovative data. The first generation of networks is trained on the relevant sequences in human speech from TIMIT. The subsequent generations are not trained on TIMIT, but on generated outputs from the previous generation and thus start learning from each other in an iterative learning task. The initial allophonic distribution is progressively being lost with each generation, likely due to pressures from the global distribution of aspiration in the training data. The networks show signs of a gradual shift in phonetic targets characteristic of a gradual phonetic sound change. At endpoints, the outputs superficially resemble a phonological change – rule loss.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusTACL" class="col-sm-8">
    
      <span class="title">Identity-Based Patterns in Deep Convolutional Networks: Generative Adversarial Phonology and Reduplication</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Transactions of the Association for Computational Linguistics</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00421/107834/Identity-Based-Patterns-in-Deep-Convolutional" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusTACL.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper models unsupervised learning of an identity-based pattern (or copying) in speech called reduplication from raw continuous data with deep convolutional neural networks. We use the ciwGAN architecture (Beguš, 2021a) in which learning of meaningful representations in speech emerges from a requirement that the CNNs generate informative data. We propose a technique to wug-test CNNs trained on speech and, based on four generative tests, argue that the network learns to represent an identity-based pattern in its latent space. By manipulating only two categorical variables in the latent space, we can actively turn an unreduplicated form into a reduplicated form with no other substantial changes to the output in the majority of cases. We also argue that the network extends the identity-based pattern to unobserved data. Exploration of how meaningful representations of identity-based patterns emerge in CNNs and how the latent space variables outside of the training range correlate with identity-based patterns in the output has general implications for neural network interpretability.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusJurgec21" class="col-sm-8">
    
      <span class="title">Tone, stress, quantity, and quality: prosodic patterns and tonal wug-tests in Žiri Slovenian</span>
      <span class="author">
        
          
            
              
                
                  Beguš, Gašper,
                
              
            
          
        
          
            
              
                
                  and Jurgec, Peter
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>Preprint</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://psyarxiv.com/b7uj5" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusJurgecZiri.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>The prosodic systems of the world’s languages vary widely in their complexity. In this paper, we report on a prosodic pattern in Žiri Slovenian, which displays an intricate set of interactions between tone, stress, quantity, and vowel quality. Some of the prosodic patterns are well motivated (e.g. the preference of High tone syllables to be stressed) while others lack phonetic or phonological motivation (e.g. the preference of long vowels to fall on the second syllable of a trochaic foot). Žiri also displays a rare case of interaction of prosody with vowel quality. To confirm productivity of the observed patterns, we conducted a wug experiment that tested the dependence of stress on vowel quality, word length, and tone. All in all, this paper brings forth new instances of phonetically unmotivated phonological processes at the suprasegmental level, which appear less discussed than at the segmental level. We also discuss methodological issues arising from artificial experiments on tonal processes.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus20" class="col-sm-8">
    
      <span class="title">Generative Adversarial Phonology: Modeling Unsupervised Phonetic and Phonological Learning With Neural Networks</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Frontiers in Artificial Intelligence</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.frontiersin.org/article/10.3389/frai.2020.00044" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusGAP.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Training deep neural networks on well-understood dependencies in speech data can provide new insights into how they learn internal representations. This paper argues that acquisition of speech can be modeled as a dependency between random space and generated speech data in the Generative Adversarial Network architecture and proposes a methodology to uncover the network’s internal representations that correspond to phonetic and phonological properties. The Generative Adversarial architecture is uniquely appropriate for modeling phonetic and phonological learning because the network is trained on unannotated raw acoustic data and learning is unsupervised without any language-specific assumptions or pre-assumed levels of abstraction. A Generative Adversarial Network was trained on an allophonic distribution in English, in which voiceless stops surface as aspirated word-initially before stressed vowels, except if preceded by a sibilant [s]. The network successfully learns the allophonic alternation: the network’s generated speech signal contains the conditional distribution of aspiration duration. The paper proposes a technique for establishing the network’s internal representations that identifies latent variables that correspond to, for example, presence of [s] and its spectral properties. By manipulating these variables, we actively control the presence of [s] and its frication amplitude in the generated outputs. This suggests that the network learns to use latent variables as an approximation of phonetic and phonological representations. Crucially, we observe that the dependencies learned in training extend beyond the training interval, which allows for additional exploration of learning representations. The paper also discusses how the network’s architecture and innovative outputs resemble and differ from linguistic behavior in language acquisition, speech disorders, and speech errors, and how well-understood dependencies in speech data can help us interpret how neural networks learn their representations.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusscil" class="col-sm-8">
    
      <span class="title">Modeling unsupervised phonetic and phonological learning in Generative Adversarial Phonology</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the Society for Computation in Linguistics</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://doi.org/10.7275/nbrf-1a27" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusScil.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper models phonetic and phonological learning as a dependency between random space and generated speech data in the Generative Adversarial Neural network architecture and proposes a methodology to uncover the network’s internal representation that corresponds to phonetic and phonological features. A Generative Adversarial Network (Goodfellow et al. 2014; implemented as WaveGAN for acoustic data by Donahue et al. 2019) was trained on an allophonic distribution in English, where voiceless stops surface as aspirated word-initially before stressed vowels except if preceded by a sibilant [s]. The network successfully learns the allophonic alternation: the network’s generated speech signal contains the conditional distribution of aspiration duration. Additionally, the network generates innovative outputs for which no evidence is available in the training data, suggesting that the network segments continuous speech signal into units that can be productively recombined. The paper also proposes a technique for establishing the network’s internal representations. We identify latent variables that directly correspond to presence of [s] in the output. By manipulating these variables, we actively control the presence of [s], its frication amplitude, and spectral shape of the frication noise in the generated outputs.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus20phonology" class="col-sm-8">
    
      <span class="title">Estimating historical probabilities of natural and unnatural processes</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Phonology</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.cambridge.org/core/journals/phonology/article/estimating-historical-probabilities-of-natural-and-unnatural-processes/CAC83CE585B82836CBC0D3A4BCDF17EB" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusPhonology.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper presents a technique for estimating the influences of channel bias on phonological typology. The technique, based on statistical bootstrapping, enables the estimation of historical probability, the probability that a synchronic alternation arises based on two diachronic factors: the number of sound changes required for an alternation to arise and their respective probabilities. I estimate historical probabilities of six attested and unattested alternations targeting the feature [voice], compare historical probabilities of these alternations, perform inferential statistics on the comparison and, to evaluate the performance of the channel bias approach, compare outputs of the diachronic model against the independently observed synchronic typology. The technique also identifies mismatches between the typological predictions of the analytic bias and channel bias approaches. By comparing these mismatches with the observed typology, this paper attempts to quantitatively evaluate the distinct contributions of the two influences on typology in a set of alternations targeting the feature [voice].</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus19" class="col-sm-8">
    
      <span class="title">Post-nasal devoicing and the blurring process</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Journal of Linguistics</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.cambridge.org/core/journals/journal-of-linguistics/article/postnasal-devoicing-and-the-blurring-process/51B1F27754D14F8BF523B905FFFE3BA1/share/527b629e44500591b52700cd181c20b09e444e49" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper addresses one of the most contested issues in phonology: unnatural alternations. First, non-natural phonological processes are subdivided into unmotivated and unnatural. The central topic of the paper is an unnatural process: post-nasal devoicing (PND). I collect thirteen cases of PND and argue that in all reported cases, PND does not derive from a single unnatural sound change (as claimed in some individual accounts of the data), but rather from a combination of three sound changes, each of which is phonetically motivated. I present new evidence showing that the three stages are directly historically attested in the pre-history of Yaghnobi. Based on several discussed cases, I propose a new diachronic model for explaining unnatural phenomena called the Blurring Process and point to its advantages over competing approaches (hypercorrection, perceptual enhancement, and phonetic motivation). The Blurring Process establishes general diachronic conditions for unnatural synchronic processes and can be employed to explain unnatural processes beyond PND. Additionally, I provide a proof establishing the minimal sound changes required for an unmotivated/unnatural alternation to arise. The Blurring Process and Minimal Sound Change Requirement have implications for models of typology within the Channel Bias approach. This paper thus presents a first step toward the ultimate goal of quantifying the influences of Channel Bias on phonological typology.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusnazarov18" class="col-sm-8">
    
      <span class="title">Gradient trends against phonetic naturalness: The case of Tarma Quechua</span>
      <span class="author">
        
          
            
              
                
                  Beguš, Gašper,
                
              
            
          
        
          
            
              
                
                  and Nazarov, Aleksei
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In NELS 48: Proceedings of the Forty-Eighth Annual Meeting of the North East Linguistic Society</em>
      
      
        2018
      
      </span>
    

    <span class="links">
    
    
    
    
      [<a href="/assets/pdf/begusNazarovNels.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus" class="col-sm-8">
    
      <span class="title">A Formal Model of Phonological Typology</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the West Coast Conference on Formal Linguistics</em>
      
      
        2017
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="http://search.proquest.com/docview/2244647771/" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begus_17_a-formal-model.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Identifying and modeling factors that influence typology has been one of the most contested issues in phonology with two major lines of thought emerging in this discussion: the Analytic Bias (AB) and Channel Bias (CB) approaches (Moreton 2008). Empirical evidence in favor of both approaches exists, yet very few attempts have been made to model them together. This paper aims to fill this gap and proposes a new MaxEnt-compatible model of phonological typology that models both AB and CB together. The first step towards a new model of typology is to establish quantitative models of each of the subcomponents: AB and CB. To encode the AB portion of the typology, we adopt Wilson’s (2006) approach of differentiating variance in the prior of a MaxEnt model of phonological learning; to encode the CB portion, we adopt Beguš’s (2016) new model of typology within CB that operates with Historical Probabilities of Alternations and an estimation method called Bootstrapping Sound Changes. This paper proposes a new model of typology that combines differentiating prior variance (AB; Wilson 2006) with estimating Historical Weights based on Historical Probabilities (CB; Beguš 2016), whereby both variables influence the typology. I further argue that this new model performs better than the current "split" models on the basis of two alternations, post-nasal voicing and devoicing, and point to future directions this line of research should take.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="doi:10.1121/1.5007728" class="col-sm-8">
    
      <span class="title">Effects of ejective stops on preceding vowel duration</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Journal of the Acoustical Society of America</em>
      
      
        2017
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://doi.org/10.1121/1.5007728" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begus_effects_of_ejective_stops_on_preceding_vowel_duration_01.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>One of the most widely studied observations in linguistic phonetics is that, all else being equal, vowels are longer before voiced than before voiceless obstruents. The causes of this phonetic generalization are, however, poorly understood and several competing explanations have been proposed. No studies have so far measured vowel duration before stops with yet another laryngeal feature: ejectives. This study fills this gap and presents results from an experiment that measures vowel duration before stops with all three laryngeal features in Georgian and models effects of both closure and voice onset time (VOT) on preceding vowel duration at the same time. The results show that vowels have significantly different durations before all three series of stops, voiced, ejective, and voiceless aspirated, even when closure and VOT durations are controlled for. The results also suggest that closure and VOT durations are inversely correlated with preceding vowel duration. These results combined bear several implications for the discussion of causes of vowel duration differences: the data support the hypotheses that claim that laryngeal gestures, temporal compensation, and closure velocity affect vowel duration. Some explanations, especially perceptual and airflow expenditure explanations, are considerably weakened by the results.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus16" class="col-sm-8">
    
      <span class="title">The phonetics of the independent svarita in Vedic</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 26th Annual UCLA Indo-European Conference</em>
      
      
        2016
      
      </span>
    

    <span class="links">
    
    
    
      [<a href="http://www.hempen-verlag.de/sprachwissenschaften/indogermanistik/ucla-proceedings/proceedings-of-the-26th-annual-ucla-indo-european-conference.html" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begus_the_phonetics_of_independent_svarita_in_vedic.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus15" class="col-sm-8">
    
      <span class="title">A new rule in vedic metrics</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Journal of the American Oriental Society</em>
      
      
        2015
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="http://dx.doi.org/10.7817/jameroriesoci.135.3.541" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begus_a_new_rule_in_vedic_metrics.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this paper I propose a new rule of Vedic meter. The glides v and y are regularly lost before the corresponding high vowels ū̆ and ī̆ in Vedic. I argue that the word-initial glides v and y before the short vowels ŭ and ĭ still “make position” and that they should be restored for metrical purposes. This means that word-final syllables of the shape -V̆C should be scanned long if the following syllable begins with a u- or i- that goes back to *vu- or *yi-. This new rule has consequences for the general metrical shape of the Rigveda, as cadences previously scanned as irregular will be repaired to their canonical shape. The rule can also be employed as etymologically decisive for words that can potentially go back to forms with or without an initial glide.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Beguš2015" class="col-sm-8">
    
      <span class="title">The circumflex advancement in prekmurje Slovenian and Bednja Kajkavian</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Zeitschrift für Slawistik</em>
      
      
        2015
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="http://dx.doi.org/10.1515/slaw-2015-0003" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>The circumflex advancement is usually dated after the loss of the weak jers. However, this chronology has been questioned by Vermeer (1979) and Greenberg (1992, 1993), who claim the opposite: that the weak jers were lost after the advancement. They further propose the “non-advancement rule”, by which the circumflex does not advance if a weak jer follows. Their evidence comes almost exclusively from the l-participles of the accentual paradigm c, which have the initial accent in the two dialects. The article presents new data that argue against this proposal. It is shown that the circumflex regularly advances in words outside the category of l-participles despite the presence of a subsequent weak jer. Moreover, a new explanation is given for the initial accent in l-participles that better captures the data. </p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2011</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus11" class="col-sm-8">
    
      <span class="title">Relativna kronologija naglasnih pojavov govora Žirovske kotline poljanskega narečja</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Slovenski jezik – Slovene Linguistic Studies</em>
      
      
        2011
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="http://dx.doi.org/10.17161/SLS.1808.7536" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begus_relativna_kronologija_naglasnih_pojavov_govora_zirovske_kotline_poljanskega_narecja.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>(translation) The Žiri Basin local dialect within the Poljane dialect evinces several special features in its structure and development. Based on the system described in Stanonik (1977), the author elucidates the system’s accentual changes from Common Slovene to the present state, and also presents some new discoveries concerning the system itself that help explain the phenomena more precisely. This description helps establish a relative chronology of the accentual phenomena, the resulting model of which is compared and contrasted with other explanations. Finally, the relative chronology of accentual changes is placed in the larger context of the development of Slovene.</p>
    </span>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Gasper Begus.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: October 30, 2022.
    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.0/umd/popper.min.js" integrity="sha256-OH05DFHUWzr725HmuHo3pnuvUUn+TJuj8/Qz9xytFEw=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/js/mdb.min.js"  integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js" integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>





<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-173393077-1', 'auto');
ga('send', 'pageview');
</script>



</html>
