---
layout: post
title: A paper accepted at Interspeech 2022
date: 2022-06-21
inline: false
---

Our paper with Alan Zhou on modeling speech recognition and synthesis simultaneously got into Interspeech 2022. 

The model learns lexical and sub-lexical (phon/n-gram) information without a direct access to training data. 

One important finding: binary codes encode holistic (lexical) info, individual bits encode featural (sublexical) info in an interpretable way (tested with a causal technique). 

Paper: [arXiv](https://arxiv.org/abs/2203.11476)

<div class="row">
<iframe width="640" height="415" src="https://youtube.com/embed/iOibJHa3WDg" frameborder="0" allowfullscreen></iframe>
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ '/assets/img/interspeech.png' | relative_url }}" alt="" title="example image"/>
    </div>
</div>
<div class="caption">
    Modeling speech recognition and synthesis simultaneously.
</div>



